---
abstract: 'Protein-protein interactions (PPIs) are essentials for many biological
  processes where two or more proteins physically bind together to achieve their functions.
  Modeling PPIs is useful for many biomedical applications, such as vaccine design,
  antibody therapeutics, and peptide drug discovery. Pre-training a protein model
  to learn effective representation is critical for PPIs. Most pre-training models
  for PPIs are sequence-based, which naively adopt the language models used in natural
  language processing to amino acid sequences. More advanced works utilize the structure-aware
  pre-training technique, taking advantage of the contact maps of known protein structures.
  However, neither sequences nor contact maps can fully characterize structures and
  functions of the proteins, which are closely related to the PPI problem. Inspired
  by this insight, we propose a multimodal protein pre-training model with three modalities:
  sequence, structure, and function (S2F). Notably, instead of using contact maps
  to learn the amino acid-level rigid structures, we encode the structure feature
  with the topology complex of point clouds of heavy atoms. It allows our model to
  learn structural information about not only the backbones but also the side chains.
  Moreover, our model incorporates the knowledge from the functional description of
  proteins extracted from literature or manual annotations. Our experiments show that
  the S2F learns protein embeddings that achieve good performances on a variety of
  PPIs tasks, including cross-species PPI, antibody-antigen affinity prediction, antibody
  neutralization prediction for SARS-CoV-2, and mutation-driven binding affinity change
  prediction.'
title: Multimodal Pre-Training Model for Sequence-based Prediction of Protein-Protein
  Interaction
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xue22a
month: 0
tex_title: Multimodal Pre-Training Model for Sequence-based Prediction of Protein-Protein
  Interaction
firstpage: 34
lastpage: 46
page: 34-46
order: 34
cycles: false
bibtex_author: Xue, Yang and Liu, Zijing and Fang, Xiaomin and Wang, Fan
author:
- given: Yang
  family: Xue
- given: Zijing
  family: Liu
- given: Xiaomin
  family: Fang
- given: Fan
  family: Wang
date: 2022-01-07
address:
container-title: Proceedings of the 16th Machine Learning in Computational Biology
  meeting
volume: '165'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 1
  - 7
pdf: https://proceedings.mlr.press/v165/xue22a/xue22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
